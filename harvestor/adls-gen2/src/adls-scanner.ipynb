{"cells":[{"cell_type":"code","source":["import os\nfrom azure.common.credentials import ServicePrincipalCredentials\nfrom azure.mgmt.resource import ResourceManagementClient\nfrom azure.mgmt.storage import StorageManagementClient\nfrom azure.storage.blob import BlockBlobService\nimport json\nimport time\nfrom datetime import datetime"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["Get all the variables from cluster's environment variables. The variables should be pushed to the cluster via DevOps pipline in a ideal scenario"],"metadata":{}},{"cell_type":"code","source":["# Get Environment varaibles\nKeyVault_Scope = os.environ['Azure_KeyVault_Scope']\nKeyVault_ADLSGen2_Access_Secret_Name = os.environ['KeyVault_ADLSGen2_Access_Secret_Name']\nADLSGen2_URL = os.environ['ADLSGen2_URL']\nADLSGen2_FileSystem = os.environ['ADLSGen2_FileSystem']\nKeyVault_BlobStorage_Access_Secret_Name = os.environ['KeyVault_BlobStorage_Access_Secret_Name']\nBlobStorage_URL = os.environ['BlobStorage_URL']\nBlobStorage_Output_Container = os.environ['BlobStorage_Output']\nScan_Depth=os.environ['Scan_Depth']\nAAD_Client_Id=os.environ['AAD_Client_Id']\nKeyVault_Client_Secret_Secret_Name=os.environ['KeyVault_Client_Secret_Secret_Name']\nADLSGen2_Resource_Group=os.environ['ADLSGen2_Resource_Group']\nADLSGen2_Subscription_Id=os.environ['ADLSGen2_Subscription_Id']\nAAD_Tenant_Id=os.environ['AAD_Tenant_Id']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["Set spart configuration to access ADLS Gen2 file system. This python job using ADLS storage keys to access the file system from Spark."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\n  \"fs.azure.account.key.\"+ADLSGen2_URL,\n  dbutils.secrets.get(scope = KeyVault_Scope, key =KeyVault_ADLSGen2_Access_Secret_Name ))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Set spark configuration to access Blob storage to store the output JSON file"],"metadata":{}},{"cell_type":"code","source":["# Set spark configuration for output Blob Storage Account\nspark.conf.set(\n  \"fs.azure.account.key.\"+BlobStorage_URL,\n dbutils.secrets.get(scope = KeyVault_Scope, key = KeyVault_BlobStorage_Access_Secret_Name))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Method to upload create file in Azure blob storage. <br>\nInputs : Content, file name"],"metadata":{}},{"cell_type":"code","source":["def uploadtoBlob(content, file_name):\n  try:\n    timenow = datetime.now()\n    file_name = file_name+str(timenow.strftime(\"-%m%d%Y-%H-%M-%S\"))+\".json\"\n    result = dbutils.fs.put(\"wasbs://\"+BlobStorage_Output_Container+\"@\"+BlobStorage_URL+\"/\"+file_name,content,True)\n    if result == True:\n      print(\"File creation success!\")\n    else:\n      print(\"File creation failed\")\n  except Exception as e:\n    print('Error occurred while creating blob', e)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Method to make JSON in a needed format <br>\nInput - Python list <br>\nOutput - Json String"],"metadata":{}},{"cell_type":"code","source":["def makeoutputjson_storageaccount(name,subscriptionId,resourceGroupName,location,createTime,SKU):\n  entity_json ={\n    \"entity_type_name\":\"azure_storage_account\",\n    \"created_by\":\"harvester\",\n     \"attributes\":[\n         {\n             \"attr_name\":\"qualifiedname\",\n             \"attr_value\":\"\"\n         },\n         {\n            \"attr_name\":\"name\",\n            \"attr_value\":name\n        },\n        {\n            \"attr_name\":\"subscriptionId\",\n            \"attr_value\":subscriptionId\n        }, \n        {\n            \"attr_name\":\"resourceGroupName\",\n            \"attr_value\":resourceGroupName\n        },\n        {\n            \"attr_name\":\"location\",\n            \"attr_value\":location\n        },\n        {\n            \"attr_name\":\"createTime\",\n            \"attr_value\":createTime\n        },\n        {\n            \"attr_name\":\"accessTier\",\n            \"attr_value\":\"Unknown\"\n        },\n        {\n            \"attr_name\":\"SKU\",\n            \"attr_value\":SKU\n        },\n        {\n            \"attr_name\":\"kind\",\n            \"attr_value\":\"StorageV2\"\n        }\n     ]\n  }\n  json_string= json.dumps(entity_json)\n  return json_string"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["def makeoutputjson_filesystem(name):\n  entity_json ={\n      \"entity_type_name\": \"azure_datalake_gen2_filesystem\",\n      \"created_by\": \"harvester\",\n      \"attributes\": [{\n          \"attr_name\": \"qualifiedName\",\n          \"attr_value\": \"\",\n          \"is_entityref\": False\n       }, \n        {\n          \"attr_name\": \"name\",\n          \"attr_value\": name,\n          \"is_entityref\": False\n      }\n      ]\n    }\n  json_string= json.dumps(entity_json)\n  return json_string\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["def makeoutputjson(entitylist):\n  entity_final=[]\n  for entity in entitylist:\n    entity_json ={\n      \"entity_type_name\": \"azure_datalake_gen2_resource_set\",\n      \"created_by\": \"harvester\",\n      \"attributes\": [{\n          \"attr_name\": \"qualifiedName\",\n          \"attr_value\": \"\",\n          \"is_entityref\": False\n       }, \n        {\n          \"attr_name\": \"name\",\n          \"attr_value\": entity,\n          \"is_entityref\": False\n      }\n      ]\n    }\n    entity_final.append(entity_json)\n  json_string= json.dumps(entity_final)\n  return json_string"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Method to scan the ADLS Gen 2 file system folders recursively using databricks dbutils"],"metadata":{}},{"cell_type":"code","source":["def getpath(path, level, entitylist, root_path ):  \n  files = dbutils.fs.ls(path)\n  for file in files:    \n    pathvalue = str(file.path)      \n    pathvalue_string = pathvalue.split(root_path)      \n    pathvalue_entity =pathvalue_string[-1]    \n    entitylist.append(pathvalue_entity)\n    if level <= int(Scan_Depth):\n        newlevel= level+1        \n        getpath(file.path,newlevel,entitylist,root_path)\n  return entitylist"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":[" \ndef scanfilesystem(filesystem_name, account_url):  \n  entitylist=[]\n  startlevel =1\n  try:    \n    root_path = \"abfss://\"+filesystem_name+\"@\"+account_url+\"/\"\n    entitylist = getpath(root_path,startlevel,entitylist,root_path)\n  except:\n    print('Error in scan file system')\n  finally:\n    return entitylist"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["<b>Main method</b> <br>\nThis script is using Azure serice principal to get access to the storage account properties <br>\nService principal client id is stored in environment variable and secrets are pulled from Azure KeyVault <br><br>\nThis script doesn't mount ADLS Gen 2 file system to databricks, instead it directly access the file system"],"metadata":{}},{"cell_type":"code","source":["subscription_id =  ADLSGen2_Subscription_Id\nad_client_id=AAD_Client_Id\nad_client_secret=dbutils.secrets.get(scope = KeyVault_Scope, key =KeyVault_Client_Secret_Secret_Name ) \nad_tenantid=AAD_Tenant_Id\n#\nresource_group_name=ADLSGen2_Resource_Group\nstorage_account_name=ADLSGen2_URL[0:ADLSGen2_URL.find('.')]\n#Make credential object\ncredentials = ServicePrincipalCredentials(client_id=ad_client_id, secret=ad_client_secret, tenant=ad_tenantid)    \nresource_client = ResourceManagementClient(credentials, subscription_id)\nstorage_client = StorageManagementClient(credentials, subscription_id)\nstorage_account = storage_client.storage_accounts.get_properties(resource_group_name, storage_account_name)\n# Get properties of storage account\n# Get properties of storage account\nsa_creation_time=storage_account.creation_time.strftime(\"%Y-%m-%d %H:%M:%S\")\nsa_kind=storage_account.kind\nsa_location=storage_account.location\nsa_name=storage_account.name\nsa_sku=storage_account.sku.name\nprint(sa_sku)\n# Go further only for Storage Gen 2\nif sa_kind =='StorageV2':\n  # make output json for storage account\n  output_json_sa = makeoutputjson_storageaccount(sa_name,subscription_id,resource_group_name,sa_location,sa_creation_time,sa_sku)\n  print(output_json_sa)\n  #Get Storage Account file system properties\n  storage_keys = storage_client.storage_accounts.list_keys(resource_group_name, storage_account_name)\n  storage_keys = {v.key_name: v.value for v in storage_keys.keys}\n  block_blob_service = BlockBlobService(account_name=storage_account_name, account_key=storage_keys['key1'])\n  containers = block_blob_service.list_containers()\n  filesystems=[]\n  # Get file system names\n  for container in containers:\n      filesystems.append(container.name)\n  for filesystem in filesystems:  \n    # Pull the entity\n    # make output json for file system\n    output_json_fs= makeoutputjson_filesystem(filesystem)    \n    #print(output_json_fs)\n    entitylist= scanfilesystem(filesystem,ADLSGen2_URL)\n    if len(entitylist) >0:\n      output_json= makeoutputjson(entitylist)\n      output_json_filesystem={\n        \"azure_storage_account\":json.loads(output_json_sa),\n        \"azure_datalake_gen2_filesystem\":json.loads(output_json_fs),\n        \"azure_datalake_gen2_resource_set\":json.loads(output_json)\n      }\n      json_string_final= json.dumps(output_json_filesystem)\n      output_filename=filesystem+\"@\"+ADLSGen2_URL\n      uploadtoBlob(json_string_final,output_filename)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Standard_RAGRS\n{&quot;created_by&quot;: &quot;harvester&quot;, &quot;entity_type_name&quot;: &quot;azure_storage_account&quot;, &quot;attributes&quot;: [{&quot;attr_value&quot;: &quot;&quot;, &quot;attr_name&quot;: &quot;qualifiedname&quot;}, {&quot;attr_value&quot;: &quot;beamdatav2&quot;, &quot;attr_name&quot;: &quot;name&quot;}, {&quot;attr_value&quot;: &quot;25d668e3-ce02-4784-9a44-924bd741f07a&quot;, &quot;attr_name&quot;: &quot;subscriptionId&quot;}, {&quot;attr_value&quot;: &quot;walmart-gbs&quot;, &quot;attr_name&quot;: &quot;resourceGroupName&quot;}, {&quot;attr_value&quot;: &quot;southeastasia&quot;, &quot;attr_name&quot;: &quot;location&quot;}, {&quot;attr_value&quot;: &quot;2019-07-08 03:47:05&quot;, &quot;attr_name&quot;: &quot;createTime&quot;}, {&quot;attr_value&quot;: &quot;Unknown&quot;, &quot;attr_name&quot;: &quot;accessTier&quot;}, {&quot;attr_value&quot;: &quot;Standard_RAGRS&quot;, &quot;attr_name&quot;: &quot;SKU&quot;}, {&quot;attr_value&quot;: &quot;StorageV2&quot;, &quot;attr_name&quot;: &quot;kind&quot;}]}\nWrote 9076 bytes.\nFile creation success!\nWrote 9077 bytes.\nFile creation success!\nWrote 9088 bytes.\nFile creation success!\nWrote 321537 bytes.\nFile creation success!\nWrote 755152 bytes.\nFile creation success!\nWrote 378056 bytes.\nFile creation success!\nWrote 396789 bytes.\nFile creation success!\nWrote 1068095 bytes.\nFile creation success!\n</div>"]}}],"execution_count":18}],"metadata":{"name":"adls-harvester-v4","notebookId":572386119231581},"nbformat":4,"nbformat_minor":0}
